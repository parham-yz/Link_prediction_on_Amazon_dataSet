{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gnn_lp_(3).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K9U1XSGyDn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff1540ce-37da-4df2-9d18-0fbc093b74a3"
      },
      "source": [
        "# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Collecting torch-scatter\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.9.0%2Bcu102/torch_scatter-2.0.7-cp37-cp37m-linux_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 4.1MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.7\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Collecting torch-sparse\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.9.0%2Bcu102/torch_sparse-0.6.10-cp37-cp37m-linux_x86_64.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.10\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Collecting torch-cluster\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.9.0%2Bcu102/torch_cluster-1.5.9-cp37-cp37m-linux_x86_64.whl (926kB)\n",
            "\u001b[K     |████████████████████████████████| 931kB 4.0MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.9\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Collecting torch-spline-conv\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.9.0%2Bcu102/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (382kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 4.1MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Collecting torch-geometric\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/61/b3f23832120c404673f6759008312ffe8269524a29bf6116d9980e44517b/torch_geometric-1.7.2.tar.gz (222kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.5.1)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.15)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.7.2-cp37-none-any.whl size=388143 sha256=ba5d89407be2e2d58a3d1bc5fe2c00cbe523bebcf56c197253c1ec5332023961\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/66/5b/ad17ef7f04b7c425dc6930daac160c3747231b0d65f9ac7972\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, torch-geometric\n",
            "Successfully installed isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.7.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5YgeiZAxx-l"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuMaTjdqyPi5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a085e114-24b0-4084-93cb-59397d9517cd"
      },
      "source": [
        "import torch_geometric.datasets\n",
        "device = torch.device('cuda:0')\n",
        "print(device)\n",
        "data = torch_geometric.datasets.amazon.Amazon('.','Computers').data.to(device)\n",
        "# data = torch_geometric.datasets.amazon.Amazon('.','Computers').data.to(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/amazon_electronics_computers.npz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ftsAbqIp2gd"
      },
      "source": [
        "class GNNEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim,output_dim):\n",
        "        super(GNNEncoder, self).__init__()\n",
        "\n",
        "        self.conv1 = pyg_nn.GCNConv(input_dim, hidden_dim*4)\n",
        "        self.conv2 = pyg_nn.GCNConv(hidden_dim*4, hidden_dim*2)\n",
        "        self.conv3 = pyg_nn.GCNConv(hidden_dim*2, hidden_dim)\n",
        "\n",
        "        self.ln1 = nn.Linear(hidden_dim*4,hidden_dim*4)\n",
        "        self.ln2 = nn.Linear(hidden_dim*2,hidden_dim*2)\n",
        "        self.ln3 = nn.Linear(hidden_dim,output_dim)\n",
        "\n",
        "        self. bias = nn.Parameter(torch.tensor(-10,dtype = float))\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(1)\n",
        "        self.dropout = 0.25\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, data, do_mult = True):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x,edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.tanh(self.ln1(x))+x\n",
        "\n",
        "        x = self.conv2(x,edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.tanh(self.ln2(x))+x\n",
        "\n",
        "        x = self.conv3(x,edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.ln3(x)\n",
        "\n",
        "        if do_mult:\n",
        "            x = (x@torch.transpose(x, 0, 1))\n",
        "            shape = x.shape\n",
        "            x = self.bn2(x.view(-1,1)).view(shape)\n",
        "            x = torch.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class GNNDecoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim,ouput_dim):\n",
        "        super(GNNDecoder, self).__init__()\n",
        "\n",
        "        self.ln11 = nn.Linear(input_dim,hidden_dim*2)\n",
        "        self.ln12 = nn.Linear(input_dim,hidden_dim*2)\n",
        "\n",
        "        self.ln2 = nn.Linear(hidden_dim*4,hidden_dim)\n",
        "        self.ln3 = nn.Linear(hidden_dim,ouput_dim)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(input_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(input_dim)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # x1 = self.bn1(x1)\n",
        "        # x2 = self.bn2(x2)\n",
        "\n",
        "        x1 = self.ln11(x1)\n",
        "        x1 = F.relu(x1)\n",
        "\n",
        "        x2 = self.ln12(x2)\n",
        "        x2 = F.relu(x2)\n",
        "\n",
        "        x = torch.cat([x1,x2],1)\n",
        "\n",
        "        x = self.ln2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.ln3(x)\n",
        "        x = torch.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def loss(pred,ture):\n",
        "    return -10*((2*ture*torch.log(pred+10e-6)).mean() + ((1-ture)*(torch.log(1-pred+10e-6))).mean())\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My-6pzblblMu"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "A = to_dense_adj(data.edge_index).to(device)\n",
        "A = A.view(A.shape[1:])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkazRWEh8tbZ"
      },
      "source": [
        "enc = GNNEncoder(data.x.shape[1],32,32*4).to(device)\n",
        "\n",
        "# loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([10]).to(device))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWyJAxwEbfM_"
      },
      "source": [
        "dec = GNNDecoder(32*4,16,1).to(device)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVdBLegb4xOd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8219a222-5e67-4be9-8e85-5ad145b578f0"
      },
      "source": [
        "opt = optim.Adam(enc.parameters(), lr=0.005)\n",
        "enc.train(True)\n",
        "\n",
        "for ep in range(8000):\n",
        "    pred = enc(data)\n",
        "\n",
        "    pre = (A*pred).sum()/pred.sum()\n",
        "    re  = (A*pred).sum()/A.sum()\n",
        "    l = loss(pred.view(-1),A.view(-1))\n",
        "        \n",
        "    opt.zero_grad()\n",
        "    l.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if ep%10 == 0:\n",
        "        print(ep,f' Loss: {float(l):.6f}  >>  {float(pre):.6f}  {float(re):.6f} ')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0  Loss: 7.841780  >>  0.004019  0.748952 \n",
            "10  Loss: 7.369096  >>  0.004130  0.751122 \n",
            "20  Loss: 7.061755  >>  0.004132  0.735101 \n",
            "30  Loss: 6.552155  >>  0.003946  0.685026 \n",
            "40  Loss: 6.141775  >>  0.003666  0.623264 \n",
            "50  Loss: 5.881571  >>  0.003513  0.583384 \n",
            "60  Loss: 5.621598  >>  0.003209  0.521065 \n",
            "70  Loss: 5.393567  >>  0.002832  0.449489 \n",
            "80  Loss: 5.197146  >>  0.002782  0.430751 \n",
            "90  Loss: 5.013518  >>  0.003010  0.453179 \n",
            "100  Loss: 4.839059  >>  0.003027  0.443550 \n",
            "110  Loss: 4.671912  >>  0.003014  0.429676 \n",
            "120  Loss: 4.512549  >>  0.003058  0.423881 \n",
            "130  Loss: 4.360133  >>  0.003070  0.413835 \n",
            "140  Loss: 4.214393  >>  0.003117  0.408645 \n",
            "150  Loss: 4.074939  >>  0.003129  0.399046 \n",
            "160  Loss: 3.941491  >>  0.003151  0.390792 \n",
            "170  Loss: 3.813767  >>  0.003171  0.382601 \n",
            "180  Loss: 3.691556  >>  0.003193  0.374817 \n",
            "190  Loss: 3.574578  >>  0.003219  0.367623 \n",
            "200  Loss: 3.462591  >>  0.003245  0.360549 \n",
            "210  Loss: 3.355352  >>  0.003274  0.354037 \n",
            "220  Loss: 3.252664  >>  0.003292  0.346488 \n",
            "230  Loss: 3.154328  >>  0.003327  0.340878 \n",
            "240  Loss: 3.060115  >>  0.003338  0.332957 \n",
            "250  Loss: 2.969859  >>  0.003354  0.325734 \n",
            "260  Loss: 2.883340  >>  0.003402  0.321832 \n",
            "270  Loss: 2.800471  >>  0.003494  0.321993 \n",
            "280  Loss: 2.722209  >>  0.003745  0.336517 \n",
            "290  Loss: 2.665740  >>  0.003151  0.274342 \n",
            "300  Loss: 2.584789  >>  0.002656  0.226133 \n",
            "310  Loss: 2.511533  >>  0.002899  0.240780 \n",
            "320  Loss: 2.442778  >>  0.002769  0.224229 \n",
            "330  Loss: 2.378021  >>  0.002747  0.216900 \n",
            "340  Loss: 2.316151  >>  0.002774  0.213646 \n",
            "350  Loss: 2.256610  >>  0.002758  0.207212 \n",
            "360  Loss: 2.199457  >>  0.002786  0.204279 \n",
            "370  Loss: 2.144514  >>  0.002777  0.198758 \n",
            "380  Loss: 2.091687  >>  0.002791  0.194999 \n",
            "390  Loss: 2.040876  >>  0.002791  0.190385 \n",
            "400  Loss: 1.991979  >>  0.002800  0.186541 \n",
            "410  Loss: 1.944912  >>  0.002805  0.182554 \n",
            "420  Loss: 1.899586  >>  0.002810  0.178698 \n",
            "430  Loss: 1.855923  >>  0.002816  0.175046 \n",
            "440  Loss: 1.813827  >>  0.002817  0.171167 \n",
            "450  Loss: 1.773234  >>  0.002824  0.167798 \n",
            "460  Loss: 1.734063  >>  0.002822  0.164053 \n",
            "470  Loss: 1.696128  >>  0.002826  0.160738 \n",
            "480  Loss: 1.659530  >>  0.002834  0.157694 \n",
            "490  Loss: 1.624169  >>  0.002846  0.155039 \n",
            "500  Loss: 1.589686  >>  0.002963  0.157855 \n",
            "510  Loss: 1.556380  >>  0.003063  0.159859 \n",
            "520  Loss: 1.524521  >>  0.003090  0.157868 \n",
            "530  Loss: 1.493721  >>  0.003095  0.154828 \n",
            "540  Loss: 1.463950  >>  0.003132  0.153521 \n",
            "550  Loss: 1.435261  >>  0.003197  0.153561 \n",
            "560  Loss: 1.407446  >>  0.003175  0.149393 \n",
            "570  Loss: 1.380630  >>  0.003139  0.144710 \n",
            "580  Loss: 1.354591  >>  0.003159  0.142763 \n",
            "590  Loss: 1.329045  >>  0.003366  0.149240 \n",
            "600  Loss: 1.372253  >>  0.004080  0.184832 \n",
            "610  Loss: 1.621275  >>  0.007843  0.388675 \n",
            "620  Loss: 1.471062  >>  0.007347  0.344048 \n",
            "630  Loss: 1.352206  >>  0.006760  0.299335 \n",
            "640  Loss: 1.270712  >>  0.006120  0.258455 \n",
            "650  Loss: 1.216182  >>  0.005511  0.223975 \n",
            "660  Loss: 1.178144  >>  0.005006  0.197187 \n",
            "670  Loss: 1.149466  >>  0.004619  0.177194 \n",
            "680  Loss: 1.125953  >>  0.004338  0.162635 \n",
            "690  Loss: 1.105300  >>  0.004150  0.152310 \n",
            "700  Loss: 1.086300  >>  0.004033  0.145130 \n",
            "710  Loss: 1.068340  >>  0.003972  0.140248 \n",
            "720  Loss: 1.051095  >>  0.003955  0.137132 \n",
            "730  Loss: 1.034402  >>  0.003975  0.135371 \n",
            "740  Loss: 1.018191  >>  0.004022  0.134634 \n",
            "750  Loss: 1.002432  >>  0.004093  0.134663 \n",
            "760  Loss: 0.987122  >>  0.004179  0.135200 \n",
            "770  Loss: 0.972257  >>  0.004275  0.136030 \n",
            "780  Loss: 0.957827  >>  0.004377  0.136997 \n",
            "790  Loss: 0.943819  >>  0.004481  0.137981 \n",
            "800  Loss: 0.930221  >>  0.004583  0.138876 \n",
            "810  Loss: 0.917011  >>  0.004682  0.139622 \n",
            "820  Loss: 0.904172  >>  0.004778  0.140217 \n",
            "830  Loss: 0.891687  >>  0.004869  0.140653 \n",
            "840  Loss: 0.879540  >>  0.004956  0.140938 \n",
            "850  Loss: 0.867718  >>  0.005038  0.141076 \n",
            "860  Loss: 0.856208  >>  0.005116  0.141066 \n",
            "870  Loss: 0.844999  >>  0.005190  0.140914 \n",
            "880  Loss: 0.834080  >>  0.005260  0.140644 \n",
            "890  Loss: 0.823438  >>  0.005326  0.140290 \n",
            "900  Loss: 0.813064  >>  0.005390  0.139861 \n",
            "910  Loss: 0.802949  >>  0.005452  0.139371 \n",
            "920  Loss: 0.793081  >>  0.005513  0.138854 \n",
            "930  Loss: 0.783453  >>  0.005573  0.138320 \n",
            "940  Loss: 0.774056  >>  0.005632  0.137770 \n",
            "950  Loss: 0.764881  >>  0.005691  0.137215 \n",
            "960  Loss: 0.755920  >>  0.005749  0.136656 \n",
            "970  Loss: 0.747164  >>  0.005808  0.136110 \n",
            "980  Loss: 0.738605  >>  0.005868  0.135572 \n",
            "990  Loss: 0.730234  >>  0.005926  0.135026 \n",
            "1000  Loss: 0.722039  >>  0.005984  0.134462 \n",
            "1010  Loss: 0.714009  >>  0.006040  0.133874 \n",
            "1020  Loss: 0.706131  >>  0.006099  0.133330 \n",
            "1030  Loss: 0.698393  >>  0.006164  0.132942 \n",
            "1040  Loss: 0.690788  >>  0.006240  0.132796 \n",
            "1050  Loss: 0.683304  >>  0.006321  0.132752 \n",
            "1060  Loss: 0.675921  >>  0.006408  0.132822 \n",
            "1070  Loss: 0.668620  >>  0.006501  0.133008 \n",
            "1080  Loss: 0.661398  >>  0.006599  0.133293 \n",
            "1090  Loss: 0.654284  >>  0.006701  0.133640 \n",
            "1100  Loss: 0.647351  >>  0.006793  0.133775 \n",
            "1110  Loss: 0.640679  >>  0.006870  0.133610 \n",
            "1120  Loss: 0.634273  >>  0.006944  0.133411 \n",
            "1130  Loss: 0.628054  >>  0.007027  0.133365 \n",
            "1140  Loss: 0.621987  >>  0.007141  0.133890 \n",
            "1150  Loss: 0.616054  >>  0.007256  0.134417 \n",
            "1160  Loss: 0.610251  >>  0.007356  0.134625 \n",
            "1170  Loss: 0.604569  >>  0.007444  0.134614 \n",
            "1180  Loss: 0.599004  >>  0.007523  0.134419 \n",
            "1190  Loss: 0.593552  >>  0.007601  0.134197 \n",
            "1200  Loss: 0.588210  >>  0.007678  0.133948 \n",
            "1210  Loss: 0.582973  >>  0.007755  0.133695 \n",
            "1220  Loss: 0.577839  >>  0.007833  0.133458 \n",
            "1230  Loss: 0.572806  >>  0.007910  0.133205 \n",
            "1240  Loss: 0.567870  >>  0.007985  0.132915 \n",
            "1250  Loss: 0.563029  >>  0.008063  0.132668 \n",
            "1260  Loss: 0.558280  >>  0.008140  0.132416 \n",
            "1270  Loss: 0.553621  >>  0.008218  0.132170 \n",
            "1280  Loss: 0.549049  >>  0.008295  0.131898 \n",
            "1290  Loss: 0.544563  >>  0.008371  0.131621 \n",
            "1300  Loss: 0.540160  >>  0.008445  0.131299 \n",
            "1310  Loss: 0.535837  >>  0.008537  0.131280 \n",
            "1320  Loss: 0.531594  >>  0.008602  0.130819 \n",
            "1330  Loss: 0.527426  >>  0.008700  0.130877 \n",
            "1340  Loss: 0.523334  >>  0.008779  0.130631 \n",
            "1350  Loss: 0.519317  >>  0.008836  0.130056 \n",
            "1360  Loss: 0.515367  >>  0.008932  0.130058 \n",
            "1370  Loss: 0.511486  >>  0.009018  0.129926 \n",
            "1380  Loss: 0.507673  >>  0.009116  0.129957 \n",
            "1390  Loss: 0.503929  >>  0.009223  0.130110 \n",
            "1400  Loss: 0.500248  >>  0.009329  0.130234 \n",
            "1410  Loss: 0.496620  >>  0.009384  0.129631 \n",
            "1420  Loss: 0.493059  >>  0.009472  0.129505 \n",
            "1430  Loss: 0.489553  >>  0.009582  0.129674 \n",
            "1440  Loss: 0.486102  >>  0.009684  0.129728 \n",
            "1450  Loss: 0.482704  >>  0.009795  0.129901 \n",
            "1460  Loss: 0.479357  >>  0.009912  0.130136 \n",
            "1470  Loss: 0.476048  >>  0.010016  0.130188 \n",
            "1480  Loss: 0.472767  >>  0.010125  0.130312 \n",
            "1490  Loss: 0.469459  >>  0.010230  0.130379 \n",
            "1500  Loss: 0.465995  >>  0.010438  0.131785 \n",
            "1510  Loss: 0.462387  >>  0.010628  0.132932 \n",
            "1520  Loss: 0.458232  >>  0.010665  0.132110 \n",
            "1530  Loss: 0.455025  >>  0.010842  0.133056 \n",
            "1540  Loss: 0.451941  >>  0.010954  0.133233 \n",
            "1550  Loss: 0.448932  >>  0.011110  0.133908 \n",
            "1560  Loss: 0.446053  >>  0.011354  0.135641 \n",
            "1570  Loss: 0.443177  >>  0.011286  0.133574 \n",
            "1580  Loss: 0.440357  >>  0.011563  0.135672 \n",
            "1590  Loss: 0.437612  >>  0.011707  0.136137 \n",
            "1600  Loss: 0.434918  >>  0.011850  0.136579 \n",
            "1610  Loss: 0.432275  >>  0.011948  0.136472 \n",
            "1620  Loss: 0.429683  >>  0.011992  0.135717 \n",
            "1630  Loss: 0.427200  >>  0.012324  0.138327 \n",
            "1640  Loss: 0.424628  >>  0.012351  0.137350 \n",
            "1650  Loss: 0.422122  >>  0.012487  0.137640 \n",
            "1660  Loss: 0.419660  >>  0.012512  0.136668 \n",
            "1670  Loss: 0.417320  >>  0.012804  0.138655 \n",
            "1680  Loss: 0.414883  >>  0.012722  0.136508 \n",
            "1690  Loss: 0.412570  >>  0.012711  0.135194 \n",
            "1700  Loss: 0.410229  >>  0.012796  0.134896 \n",
            "1710  Loss: 0.407975  >>  0.013207  0.138116 \n",
            "1720  Loss: 0.405759  >>  0.013010  0.134801 \n",
            "1730  Loss: 0.403558  >>  0.013274  0.136455 \n",
            "1740  Loss: 0.401271  >>  0.013207  0.134527 \n",
            "1750  Loss: 0.399104  >>  0.013744  0.138943 \n",
            "1760  Loss: 0.397452  >>  0.014487  0.145460 \n",
            "1770  Loss: 0.395720  >>  0.012864  0.127584 \n",
            "1780  Loss: 0.393181  >>  0.013245  0.130277 \n",
            "1790  Loss: 0.390873  >>  0.014287  0.139629 \n",
            "1800  Loss: 0.388701  >>  0.014066  0.136257 \n",
            "1810  Loss: 0.387074  >>  0.014562  0.140051 \n",
            "1820  Loss: 0.385795  >>  0.014035  0.133675 \n",
            "1830  Loss: 0.383006  >>  0.014728  0.139311 \n",
            "1840  Loss: 0.381639  >>  0.014016  0.131220 \n",
            "1850  Loss: 0.379255  >>  0.014857  0.138133 \n",
            "1860  Loss: 0.376721  >>  0.014816  0.136652 \n",
            "1870  Loss: 0.378941  >>  0.015736  0.144454 \n",
            "1880  Loss: 0.377975  >>  0.015121  0.137307 \n",
            "1890  Loss: 0.373955  >>  0.015740  0.141591 \n",
            "1900  Loss: 0.371361  >>  0.014855  0.132104 \n",
            "1910  Loss: 0.369131  >>  0.014878  0.131271 \n",
            "1920  Loss: 0.366912  >>  0.015250  0.133674 \n",
            "1930  Loss: 0.365365  >>  0.015540  0.135367 \n",
            "1940  Loss: 0.364286  >>  0.016412  0.142384 \n",
            "1950  Loss: 0.370932  >>  0.015128  0.129919 \n",
            "1960  Loss: 0.366061  >>  0.016245  0.138561 \n",
            "1970  Loss: 0.361308  >>  0.016119  0.136063 \n",
            "1980  Loss: 0.359669  >>  0.016261  0.136176 \n",
            "1990  Loss: 0.357241  >>  0.015691  0.130102 \n",
            "2000  Loss: 0.355396  >>  0.016474  0.135903 \n",
            "2010  Loss: 0.353403  >>  0.016220  0.132831 \n",
            "2020  Loss: 0.351509  >>  0.017058  0.139110 \n",
            "2030  Loss: 0.349886  >>  0.016629  0.134575 \n",
            "2040  Loss: 0.348199  >>  0.016763  0.134747 \n",
            "2050  Loss: 0.354291  >>  0.018259  0.146520 \n",
            "2060  Loss: 0.354050  >>  0.016032  0.126579 \n",
            "2070  Loss: 0.351083  >>  0.016755  0.131145 \n",
            "2080  Loss: 0.348024  >>  0.017018  0.132348 \n",
            "2090  Loss: 0.343466  >>  0.017877  0.137912 \n",
            "2100  Loss: 0.341325  >>  0.018384  0.141037 \n",
            "2110  Loss: 0.339694  >>  0.018382  0.139950 \n",
            "2120  Loss: 0.338845  >>  0.017989  0.135945 \n",
            "2130  Loss: 0.338943  >>  0.018542  0.139543 \n",
            "2140  Loss: 0.335541  >>  0.018013  0.134418 \n",
            "2150  Loss: 0.335301  >>  0.017363  0.128495 \n",
            "2160  Loss: 0.332510  >>  0.018655  0.137444 \n",
            "2170  Loss: 0.333301  >>  0.020702  0.152468 \n",
            "2180  Loss: 0.329836  >>  0.018422  0.133791 \n",
            "2190  Loss: 0.327890  >>  0.020694  0.150121 \n",
            "2200  Loss: 0.326339  >>  0.020182  0.145294 \n",
            "2210  Loss: 0.329702  >>  0.019750  0.141530 \n",
            "2220  Loss: 0.324991  >>  0.019344  0.137011 \n",
            "2230  Loss: 0.325669  >>  0.021346  0.151051 \n",
            "2240  Loss: 0.323501  >>  0.022274  0.156581 \n",
            "2250  Loss: 0.320492  >>  0.020594  0.142994 \n",
            "2260  Loss: 0.321577  >>  0.019218  0.132419 \n",
            "2270  Loss: 0.320482  >>  0.020974  0.144063 \n",
            "2280  Loss: 0.320497  >>  0.019343  0.131627 \n",
            "2290  Loss: 0.316015  >>  0.021944  0.148638 \n",
            "2300  Loss: 0.315532  >>  0.021201  0.142538 \n",
            "2310  Loss: 0.313906  >>  0.023196  0.155600 \n",
            "2320  Loss: 0.312042  >>  0.022284  0.148295 \n",
            "2330  Loss: 0.312794  >>  0.021926  0.145198 \n",
            "2340  Loss: 0.313805  >>  0.023552  0.155885 \n",
            "2350  Loss: 0.311668  >>  0.021561  0.140833 \n",
            "2360  Loss: 0.311914  >>  0.022959  0.149293 \n",
            "2370  Loss: 0.308090  >>  0.022451  0.144532 \n",
            "2380  Loss: 0.306621  >>  0.024004  0.154205 \n",
            "2390  Loss: 0.305057  >>  0.024052  0.153624 \n",
            "2400  Loss: 0.305218  >>  0.023110  0.146666 \n",
            "2410  Loss: 0.309593  >>  0.023237  0.147212 \n",
            "2420  Loss: 0.306641  >>  0.023846  0.149993 \n",
            "2430  Loss: 0.302089  >>  0.025118  0.156913 \n",
            "2440  Loss: 0.301071  >>  0.023976  0.148489 \n",
            "2450  Loss: 0.300735  >>  0.024878  0.153588 \n",
            "2460  Loss: 0.300818  >>  0.025807  0.158877 \n",
            "2470  Loss: 0.298222  >>  0.025800  0.157478 \n",
            "2480  Loss: 0.297261  >>  0.025633  0.155407 \n",
            "2490  Loss: 0.297462  >>  0.025179  0.151726 \n",
            "2500  Loss: 0.297072  >>  0.024010  0.143304 \n",
            "2510  Loss: 0.294116  >>  0.026694  0.159183 \n",
            "2520  Loss: 0.298558  >>  0.024833  0.146958 \n",
            "2530  Loss: 0.294905  >>  0.023520  0.137412 \n",
            "2540  Loss: 0.291831  >>  0.026649  0.155754 \n",
            "2550  Loss: 0.291276  >>  0.027275  0.158914 \n",
            "2560  Loss: 0.290646  >>  0.027144  0.157340 \n",
            "2570  Loss: 0.290582  >>  0.029520  0.171185 \n",
            "2580  Loss: 0.290408  >>  0.025822  0.147461 \n",
            "2590  Loss: 0.291036  >>  0.027801  0.158763 \n",
            "2600  Loss: 0.289776  >>  0.026436  0.149295 \n",
            "2610  Loss: 0.288157  >>  0.027097  0.152114 \n",
            "2620  Loss: 0.285428  >>  0.027939  0.156059 \n",
            "2630  Loss: 0.286905  >>  0.026875  0.149360 \n",
            "2640  Loss: 0.283759  >>  0.029309  0.162632 \n",
            "2650  Loss: 0.282992  >>  0.028062  0.154337 \n",
            "2660  Loss: 0.286642  >>  0.030261  0.167041 \n",
            "2670  Loss: 0.281583  >>  0.028406  0.154531 \n",
            "2680  Loss: 0.281120  >>  0.029920  0.162550 \n",
            "2690  Loss: 0.279941  >>  0.029160  0.157161 \n",
            "2700  Loss: 0.279288  >>  0.029645  0.159316 \n",
            "2710  Loss: 0.278757  >>  0.029820  0.159317 \n",
            "2720  Loss: 0.279967  >>  0.030842  0.164606 \n",
            "2730  Loss: 0.278590  >>  0.032016  0.170118 \n",
            "2740  Loss: 0.277285  >>  0.028671  0.149992 \n",
            "2750  Loss: 0.282011  >>  0.025053  0.130291 \n",
            "2760  Loss: 0.282244  >>  0.030394  0.158340 \n",
            "2770  Loss: 0.278856  >>  0.030125  0.155295 \n",
            "2780  Loss: 0.277738  >>  0.026437  0.134222 \n",
            "2790  Loss: 0.275609  >>  0.031058  0.158183 \n",
            "2800  Loss: 0.273272  >>  0.032049  0.162621 \n",
            "2810  Loss: 0.272415  >>  0.030949  0.156075 \n",
            "2820  Loss: 0.272070  >>  0.031368  0.157925 \n",
            "2830  Loss: 0.271074  >>  0.031650  0.158710 \n",
            "2840  Loss: 0.270181  >>  0.032320  0.161611 \n",
            "2850  Loss: 0.269357  >>  0.032341  0.160918 \n",
            "2860  Loss: 0.269820  >>  0.030749  0.151944 \n",
            "2870  Loss: 0.269800  >>  0.033199  0.163732 \n",
            "2880  Loss: 0.266940  >>  0.033215  0.162481 \n",
            "2890  Loss: 0.265806  >>  0.034867  0.170930 \n",
            "2900  Loss: 0.264602  >>  0.034637  0.168917 \n",
            "2910  Loss: 0.267778  >>  0.032845  0.159942 \n",
            "2920  Loss: 0.265491  >>  0.032656  0.157551 \n",
            "2930  Loss: 0.279095  >>  0.031078  0.150990 \n",
            "2940  Loss: 0.273188  >>  0.029715  0.140872 \n",
            "2950  Loss: 0.265893  >>  0.034512  0.163514 \n",
            "2960  Loss: 0.276320  >>  0.034101  0.162498 \n",
            "2970  Loss: 0.267507  >>  0.032660  0.151557 \n",
            "2980  Loss: 0.262721  >>  0.032090  0.146765 \n",
            "2990  Loss: 0.260350  >>  0.035760  0.164938 \n",
            "3000  Loss: 0.259279  >>  0.035158  0.161641 \n",
            "3010  Loss: 0.258651  >>  0.035059  0.160785 \n",
            "3020  Loss: 0.257672  >>  0.037445  0.172311 \n",
            "3030  Loss: 0.256085  >>  0.037409  0.171441 \n",
            "3040  Loss: 0.255225  >>  0.038354  0.176083 \n",
            "3050  Loss: 0.255901  >>  0.038358  0.176166 \n",
            "3060  Loss: 0.253739  >>  0.039343  0.179771 \n",
            "3070  Loss: 0.253355  >>  0.038809  0.176118 \n",
            "3080  Loss: 0.251668  >>  0.039728  0.179749 \n",
            "3090  Loss: 0.252418  >>  0.040148  0.181261 \n",
            "3100  Loss: 0.251922  >>  0.038678  0.173285 \n",
            "3110  Loss: 0.252169  >>  0.041144  0.185563 \n",
            "3120  Loss: 0.249143  >>  0.040825  0.181937 \n",
            "3130  Loss: 0.248255  >>  0.041203  0.183826 \n",
            "3140  Loss: 0.247722  >>  0.043189  0.192853 \n",
            "3150  Loss: 0.247828  >>  0.043110  0.193162 \n",
            "3160  Loss: 0.247110  >>  0.041318  0.182958 \n",
            "3170  Loss: 0.245185  >>  0.043360  0.191045 \n",
            "3180  Loss: 0.243845  >>  0.042940  0.187837 \n",
            "3190  Loss: 0.243946  >>  0.043164  0.188338 \n",
            "3200  Loss: 0.242084  >>  0.045005  0.195742 \n",
            "3210  Loss: 0.241625  >>  0.045941  0.199414 \n",
            "3220  Loss: 0.240774  >>  0.045623  0.196704 \n",
            "3230  Loss: 0.241572  >>  0.044959  0.193397 \n",
            "3240  Loss: 0.241745  >>  0.046579  0.199912 \n",
            "3250  Loss: 0.239688  >>  0.044868  0.189793 \n",
            "3260  Loss: 0.239320  >>  0.045236  0.190903 \n",
            "3270  Loss: 0.237854  >>  0.046446  0.195526 \n",
            "3280  Loss: 0.237619  >>  0.046642  0.196448 \n",
            "3290  Loss: 0.237715  >>  0.047276  0.198212 \n",
            "3300  Loss: 0.237502  >>  0.047603  0.199147 \n",
            "3310  Loss: 0.236779  >>  0.045314  0.187904 \n",
            "3320  Loss: 0.236228  >>  0.049477  0.206114 \n",
            "3330  Loss: 0.234279  >>  0.049230  0.203807 \n",
            "3340  Loss: 0.233865  >>  0.047223  0.193213 \n",
            "3350  Loss: 0.233021  >>  0.050607  0.208749 \n",
            "3360  Loss: 0.232933  >>  0.048040  0.196217 \n",
            "3370  Loss: 0.231779  >>  0.050280  0.205179 \n",
            "3380  Loss: 0.231651  >>  0.049368  0.199773 \n",
            "3390  Loss: 0.230857  >>  0.050984  0.207340 \n",
            "3400  Loss: 0.231659  >>  0.051538  0.208625 \n",
            "3410  Loss: 0.229954  >>  0.052600  0.213473 \n",
            "3420  Loss: 0.229395  >>  0.050812  0.203221 \n",
            "3430  Loss: 0.228728  >>  0.051995  0.208414 \n",
            "3440  Loss: 0.231591  >>  0.049481  0.196844 \n",
            "3450  Loss: 0.228682  >>  0.051235  0.203260 \n",
            "3460  Loss: 0.227046  >>  0.052402  0.207300 \n",
            "3470  Loss: 0.228460  >>  0.052455  0.208687 \n",
            "3480  Loss: 0.225880  >>  0.052812  0.207141 \n",
            "3490  Loss: 0.225205  >>  0.054432  0.214112 \n",
            "3500  Loss: 0.225048  >>  0.052981  0.207011 \n",
            "3510  Loss: 0.225264  >>  0.055533  0.218684 \n",
            "3520  Loss: 0.224489  >>  0.052855  0.205460 \n",
            "3530  Loss: 0.223003  >>  0.056732  0.221784 \n",
            "3540  Loss: 0.221803  >>  0.055418  0.214084 \n",
            "3550  Loss: 0.221374  >>  0.055678  0.215221 \n",
            "3560  Loss: 0.221025  >>  0.057348  0.221917 \n",
            "3570  Loss: 0.220643  >>  0.057877  0.222427 \n",
            "3580  Loss: 0.219304  >>  0.057359  0.218927 \n",
            "3590  Loss: 0.220393  >>  0.056457  0.215298 \n",
            "3600  Loss: 0.218110  >>  0.060646  0.231710 \n",
            "3610  Loss: 0.216932  >>  0.058870  0.221868 \n",
            "3620  Loss: 0.218352  >>  0.056588  0.211734 \n",
            "3630  Loss: 0.216552  >>  0.061561  0.231411 \n",
            "3640  Loss: 0.218436  >>  0.060576  0.228011 \n",
            "3650  Loss: 0.214892  >>  0.060715  0.225987 \n",
            "3660  Loss: 0.214306  >>  0.062167  0.231014 \n",
            "3670  Loss: 0.214055  >>  0.062624  0.231964 \n",
            "3680  Loss: 0.213658  >>  0.062303  0.229469 \n",
            "3690  Loss: 0.212743  >>  0.063450  0.234045 \n",
            "3700  Loss: 0.213266  >>  0.062088  0.227674 \n",
            "3710  Loss: 0.215454  >>  0.060245  0.219028 \n",
            "3720  Loss: 0.211459  >>  0.064521  0.235798 \n",
            "3730  Loss: 0.210658  >>  0.064724  0.234689 \n",
            "3740  Loss: 0.209817  >>  0.065196  0.235897 \n",
            "3750  Loss: 0.210050  >>  0.064605  0.232908 \n",
            "3760  Loss: 0.209722  >>  0.063296  0.226633 \n",
            "3770  Loss: 0.211254  >>  0.063590  0.228449 \n",
            "3780  Loss: 0.209043  >>  0.067862  0.243632 \n",
            "3790  Loss: 0.210230  >>  0.061581  0.216634 \n",
            "3800  Loss: 0.208692  >>  0.063860  0.224183 \n",
            "3810  Loss: 0.208639  >>  0.068038  0.241991 \n",
            "3820  Loss: 0.209207  >>  0.066866  0.235421 \n",
            "3830  Loss: 0.207784  >>  0.067270  0.237116 \n",
            "3840  Loss: 0.205810  >>  0.068533  0.239806 \n",
            "3850  Loss: 0.205449  >>  0.068333  0.237850 \n",
            "3860  Loss: 0.204464  >>  0.070159  0.244865 \n",
            "3870  Loss: 0.203924  >>  0.069905  0.242671 \n",
            "3880  Loss: 0.203731  >>  0.070767  0.245810 \n",
            "3890  Loss: 0.203744  >>  0.070656  0.244025 \n",
            "3900  Loss: 0.205442  >>  0.069336  0.240395 \n",
            "3910  Loss: 0.205230  >>  0.070985  0.244163 \n",
            "3920  Loss: 0.204454  >>  0.070168  0.240629 \n",
            "3930  Loss: 0.202036  >>  0.070618  0.239426 \n",
            "3940  Loss: 0.205080  >>  0.068494  0.233051 \n",
            "3950  Loss: 0.210719  >>  0.064966  0.219946 \n",
            "3960  Loss: 0.203228  >>  0.071219  0.240193 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5ace07e537c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfedE3qi2nHm",
        "outputId": "68fc459b-2606-42ec-80e5-0bb2caf058a9"
      },
      "source": [
        "opt = optim.Adam(dec.parameters(), lr=0.00005)\n",
        "loss = nn.BCELoss()\n",
        "batch_size = 256*16\n",
        "enc.train(False)\n",
        "emb = enc(data,False)\n",
        "emb = emb.detach()\n",
        "\n",
        "emb = emb/emb.var() - emb.mean()\n",
        "\n",
        "for ep in range(1000):\n",
        "    \n",
        "\n",
        "    indices = torch.randint(0,data.edge_index.shape[1],(int(batch_size/2),))\n",
        "    x1 = emb[data.edge_index[0,indices]]\n",
        "    x2 = emb[data.edge_index[1,indices]]\n",
        "    \n",
        "\n",
        "\n",
        "    num_neg_smp = 0\n",
        "    tx1 = []\n",
        "    tx2 = []\n",
        "    for i,j in torch.randint(0,emb.shape[0],(batch_size*4,2)):\n",
        "        if A[i,j] == 0 and num_neg_smp < batch_size/2:\n",
        "            tx1.append(emb[i])\n",
        "            tx2.append(emb[j])\n",
        "            num_neg_smp += 1\n",
        "        if num_neg_smp > batch_size/2:\n",
        "            break\n",
        "\n",
        "    perm = torch.randperm(batch_size)\n",
        "    x1 = torch.cat([x1,torch.stack(tx1).to(device)],0)\n",
        "    x1 = x1[perm]\n",
        "    x2 = torch.cat([x2,torch.stack(tx2).to(device)],0)\n",
        "    x2 = x2[perm]\n",
        "    \n",
        "    y = torch.cat([torch.ones([int(batch_size/2)]),torch.zeros([int(batch_size/2)])],0).to(device)\n",
        "    y = y[perm]\n",
        " \n",
        "\n",
        "    pred1 = dec(x1,x2).view(-1)\n",
        "    pred2 = dec(x2,x1).view(-1)\n",
        "    # print(pred1.min(),pred1.max())\n",
        "    opt.zero_grad()\n",
        "    # pre = (y*pred1).sum()/(pred1.sum()+10e-6)\n",
        "    # re  = (y*pred1).sum()/(y.sum()+10e-6)\n",
        "    # l = -(re*pre)/(pre+ re)\n",
        "    l = loss(pred1,y) + loss(pred2,y)\n",
        "\n",
        "    l.backward()\n",
        "        \n",
        "    opt.step()\n",
        "\n",
        "    if ep%10 == 0:\n",
        "        pre = (y*pred1).sum()/(pred1.sum()+10e-6)\n",
        "        re  = (y*pred1).sum()/(y.sum()+10e-6)\n",
        "        print(ep,f' Loss: {float(l):.6f}  >>  {float(pre):.6f}  {float(re):.6f} ')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0  Loss: 0.587055  >>  0.812423  0.806773 \n",
            "10  Loss: 0.623688  >>  0.800251  0.794997 \n",
            "20  Loss: 0.625682  >>  0.804427  0.798473 \n",
            "30  Loss: 0.584389  >>  0.815027  0.803516 \n",
            "40  Loss: 0.656750  >>  0.797663  0.795675 \n",
            "50  Loss: 0.587456  >>  0.808156  0.813364 \n",
            "60  Loss: 0.638162  >>  0.796838  0.791502 \n",
            "70  Loss: 0.648301  >>  0.798887  0.793970 \n",
            "80  Loss: 0.641511  >>  0.798966  0.792416 \n",
            "90  Loss: 0.600198  >>  0.800879  0.808404 \n",
            "100  Loss: 0.627120  >>  0.801716  0.795159 \n",
            "110  Loss: 0.621997  >>  0.800191  0.802906 \n",
            "120  Loss: 0.630683  >>  0.799267  0.797689 \n",
            "130  Loss: 0.596155  >>  0.803791  0.808712 \n",
            "140  Loss: 0.630822  >>  0.797475  0.797440 \n",
            "150  Loss: 0.603256  >>  0.807694  0.798866 \n",
            "160  Loss: 0.622459  >>  0.796801  0.802240 \n",
            "170  Loss: 0.617845  >>  0.802185  0.803489 \n",
            "180  Loss: 0.661157  >>  0.791817  0.801703 \n",
            "190  Loss: 0.610825  >>  0.804092  0.796150 \n",
            "200  Loss: 0.600653  >>  0.807569  0.795699 \n",
            "210  Loss: 0.629090  >>  0.795383  0.806847 \n",
            "220  Loss: 0.564348  >>  0.814341  0.806838 \n",
            "230  Loss: 0.627517  >>  0.798306  0.799411 \n",
            "240  Loss: 0.615291  >>  0.796978  0.802816 \n",
            "250  Loss: 0.617611  >>  0.803714  0.800429 \n",
            "260  Loss: 0.626273  >>  0.794398  0.803006 \n",
            "270  Loss: 0.615938  >>  0.798556  0.810508 \n",
            "280  Loss: 0.614726  >>  0.798629  0.801984 \n",
            "290  Loss: 0.613515  >>  0.797272  0.808144 \n",
            "300  Loss: 0.614957  >>  0.803595  0.801720 \n",
            "310  Loss: 0.610179  >>  0.798928  0.808737 \n",
            "320  Loss: 0.626193  >>  0.801615  0.799899 \n",
            "330  Loss: 0.596003  >>  0.804444  0.805291 \n",
            "340  Loss: 0.632410  >>  0.799588  0.799022 \n",
            "350  Loss: 0.607976  >>  0.801790  0.807714 \n",
            "360  Loss: 0.584218  >>  0.806828  0.807802 \n",
            "370  Loss: 0.609402  >>  0.807747  0.798903 \n",
            "380  Loss: 0.598666  >>  0.808780  0.802899 \n",
            "390  Loss: 0.627981  >>  0.796525  0.797354 \n",
            "400  Loss: 0.628277  >>  0.800358  0.801766 \n",
            "410  Loss: 0.625494  >>  0.801726  0.801739 \n",
            "420  Loss: 0.642952  >>  0.792634  0.803934 \n",
            "430  Loss: 0.619829  >>  0.799964  0.808622 \n",
            "440  Loss: 0.617132  >>  0.795047  0.806664 \n",
            "450  Loss: 0.584796  >>  0.808201  0.809019 \n",
            "460  Loss: 0.606680  >>  0.800155  0.808383 \n",
            "470  Loss: 0.625358  >>  0.797515  0.806934 \n",
            "480  Loss: 0.608029  >>  0.801503  0.807948 \n",
            "490  Loss: 0.579984  >>  0.811893  0.810886 \n",
            "500  Loss: 0.631578  >>  0.796755  0.801696 \n",
            "510  Loss: 0.615868  >>  0.800583  0.806197 \n",
            "520  Loss: 0.609792  >>  0.799388  0.807245 \n",
            "530  Loss: 0.612363  >>  0.803144  0.804550 \n",
            "540  Loss: 0.600146  >>  0.803844  0.811052 \n",
            "550  Loss: 0.613348  >>  0.801533  0.805040 \n",
            "560  Loss: 0.605293  >>  0.803841  0.809717 \n",
            "570  Loss: 0.647108  >>  0.797723  0.796002 \n",
            "580  Loss: 0.639302  >>  0.796893  0.803508 \n",
            "590  Loss: 0.614730  >>  0.801705  0.801181 \n",
            "600  Loss: 0.602424  >>  0.804931  0.807169 \n",
            "610  Loss: 0.617243  >>  0.808183  0.798190 \n",
            "620  Loss: 0.611405  >>  0.805590  0.803700 \n",
            "630  Loss: 0.642683  >>  0.792408  0.801761 \n",
            "640  Loss: 0.605820  >>  0.805937  0.798281 \n",
            "650  Loss: 0.601768  >>  0.801328  0.806160 \n",
            "660  Loss: 0.596733  >>  0.803481  0.802728 \n",
            "670  Loss: 0.607208  >>  0.802934  0.813286 \n",
            "680  Loss: 0.636517  >>  0.798706  0.802780 \n",
            "690  Loss: 0.619611  >>  0.798627  0.810984 \n",
            "700  Loss: 0.612718  >>  0.799163  0.803098 \n",
            "710  Loss: 0.618776  >>  0.797645  0.801840 \n",
            "720  Loss: 0.634353  >>  0.795883  0.803036 \n",
            "730  Loss: 0.585715  >>  0.811760  0.806406 \n",
            "740  Loss: 0.605417  >>  0.806401  0.804350 \n",
            "750  Loss: 0.594448  >>  0.807921  0.811940 \n",
            "760  Loss: 0.602147  >>  0.806710  0.803538 \n",
            "770  Loss: 0.635340  >>  0.799512  0.797062 \n",
            "780  Loss: 0.615275  >>  0.809817  0.797066 \n",
            "790  Loss: 0.597478  >>  0.808239  0.808222 \n",
            "800  Loss: 0.615670  >>  0.803629  0.804966 \n",
            "810  Loss: 0.602547  >>  0.803464  0.804531 \n",
            "820  Loss: 0.633858  >>  0.792124  0.806682 \n",
            "830  Loss: 0.574121  >>  0.811773  0.805645 \n",
            "840  Loss: 0.638335  >>  0.797592  0.792569 \n",
            "850  Loss: 0.594481  >>  0.812926  0.805084 \n",
            "860  Loss: 0.645331  >>  0.798741  0.806438 \n",
            "870  Loss: 0.586664  >>  0.813350  0.798419 \n",
            "880  Loss: 0.600351  >>  0.805304  0.806369 \n",
            "890  Loss: 0.623236  >>  0.800518  0.805075 \n",
            "900  Loss: 0.624153  >>  0.805312  0.802267 \n",
            "910  Loss: 0.617448  >>  0.801566  0.810189 \n",
            "920  Loss: 0.613273  >>  0.801624  0.804037 \n",
            "930  Loss: 0.625749  >>  0.798563  0.805471 \n",
            "940  Loss: 0.599923  >>  0.808191  0.808747 \n",
            "950  Loss: 0.599717  >>  0.813796  0.804371 \n",
            "960  Loss: 0.613500  >>  0.805002  0.802410 \n",
            "970  Loss: 0.608951  >>  0.807113  0.803868 \n",
            "980  Loss: 0.631965  >>  0.803311  0.797696 \n",
            "990  Loss: 0.584523  >>  0.812807  0.804857 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r188blkL9l-H",
        "outputId": "cd643438-59c6-4ba3-9d3a-e62717fa32c5"
      },
      "source": [
        "y.mean()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5000, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "SMkyi3omg-Ai",
        "outputId": "8b7acccb-ccf1-4112-a930-56d8f8823dad"
      },
      "source": [
        "import tqdm \n",
        "A.cpu()\n",
        "AP = torch.empty(A.shape)\n",
        "for i in tqdm.trange(len(data.x)):      \n",
        "    A[i,:] = dec(torch.stack([emb[i]]*len(data.x)),emb).view(-1)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/3327 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-3f440bfddf52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-96-87136c177d9e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# x2 = self.bn2(x2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 11.17 GiB total capacity; 10.22 GiB already allocated; 832.00 KiB free; 10.71 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02bqSHUZKi5L",
        "outputId": "2e2a0940-9ce0-40f8-caf1-00eb95c7b23b"
      },
      "source": [
        "AP*(1-A).sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBkmCsLIEYZG"
      },
      "source": [
        "        pred = enc(data).view(-1) >0.9\n",
        "pre = (A*pred).sum()/pred.sum()\n",
        "re  = (A*pred).sum()/A.sum()\n",
        "pre, re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY79IK94N3RK"
      },
      "source": [
        "f'Loss: {float(1):.2f} >>  {float(1):.2f} {float(1):.2f}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0GqTJ5IgULz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "724bcca4-ef9f-447e-b528-2c67dbbee180"
      },
      "source": [
        "pred = torch.where(model(data)>0.3,1,0)\n",
        "\n",
        "pre = (A*pred).sum()/pred.sum()\n",
        "re  = (A*pred).sum()/A.sum()\n",
        "# pre* re/(pre + re)\n",
        "pre,re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-bd5af81e64b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mre\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# pre* re/(pre + re)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4p4SdDG6juq",
        "outputId": "53e3c38d-ccfd-4229-d24d-e4bf10b2b220"
      },
      "source": [
        "torch.randperm(9)[:-(9%2)].view(-1,2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 7],\n",
              "        [3, 5],\n",
              "        [2, 8],\n",
              "        [4, 6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    }
  ]
}